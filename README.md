# âš™ï¸ Project : Modern Data ELT pipele for e-commerce dataset, olist, with dbt, BigQuery and Airflow (Cloud Composer)

End-to-End Production-Grade Orchestration on Google Cloud

This project implements a complete modern data engineering pipeline, orchestrated with Airflow running on Google Cloud Composer, transforming data using dbt Core, and storing curated data in BigQuery.

The pipeline is built to match real production standards used by data teams.

# ğŸ§±ğŸ—ï¸ Architecture Overview

```yml
Google Cloud Composer (Airflow)
        â”‚
        â”‚  Schedules + Orchestrates
        â–¼
Airflow DAG â†’ BashOperator â†’ dbt Core
        â”‚
        â”‚  SQL Transformations + Tests
        â–¼
 BigQuery (raw â†’ staging â†’ marts)
        â”‚
        â–¼
 Curated analytical tables

```

â˜ï¸ GCS and Cloud Composer : Assets are synchronized through GCS buckets used by Cloud Composer

```yml
/home/airflow/gcs/dags/      â†’ Airflow DAG code
/home/airflow/gcs/plugins/   â†’ dbt + Python dependencies
/home/airflow/gcs/data/      â†’ dbt project (models/, sources/, manifest.json)
```

# ğŸ¯ Target

- Build a fully **automated ELT pipeline** using **dbt**

- **Orchestrate** dbt models with **Airflow** on **Cloud Composer**

- Use **BigQuery** as the warehouse for both raw and transformed data

- Implement **service account** impersonation, OAuth, and IAM best practices

- Solve real **production** issues in distributed cloud environments

- Produce an **end-to-end** portfolio-ready **data engineering** system

# ğŸ§® Features


âœ´ï¸ Cloud Composer (managed Airflow)

âœ´ï¸ GCS bucket syncing for code deployment

âœ´ï¸ Fully automated dbt execution in DAG

âœ´ï¸ BigQuery sources, staging, and marts

âœ´ï¸ Incremental model dependency resolution using manifest.json

âœ´ï¸ IAM-secure service account impersonation

âœ´ï¸ End-to-end logs, retries, and failure handling

----

# ğŸ”§ Cloud Components Used

| Component          | Purpose                                |
| ------------------ | -------------------------------------- |
| **Cloud Composer** | Orchestration (managed Airflow)        |
| **BigQuery**       | Data warehouse                         |
| **dbt Core**       | SQL transformations, lineage, tests    |
| **GCS Buckets**    | Store DAGs, dbt project, logs          |
| **IAM**            | Secure authentication between services |

# ğŸ“ Folder Structure in GCS


```yml
gs://<composer-bucket>/dags/
    â”œâ”€â”€ dbt_dag.py
    â”œâ”€â”€ models/
        â”œâ”€â”€ marts/
            â”œâ”€â”€dim_product_translated_name.sql
        â”œâ”€â”€ staging/
            â”œâ”€â”€stg_olist_product_src.sql
            â”œâ”€â”€stg_translated_product_name_src.sql

    â”œâ”€â”€ macros/
        â”œâ”€â”€ norm_key.sql
    â”œâ”€â”€ dbt_project.yml
    â””â”€â”€ _src.yml


gs://<raw-data-bucket>/data/
    â”œâ”€â”€ profiles/
        â”œâ”€â”€ profiles.yml
    â”œâ”€â”€ target/
        â”œâ”€â”€manifest.json

```
# TODO change this title
# What I Successfully Built (What Engineers Care About)

#### 1. Cloud Composer Environment Setup
I fully configured Composer, including:

- Environment creation in europe-west1
- Determining high-resilience vs standard resilience
- Installing Python/dbt dependencies through PyPI
- Configuring Composer Workers to run dbt


#### 2. GCS Bucket Architecture

| Folder     | Purpose                        |
| ---------- | ------------------------------ |
| `dags/`    | DAG files + dbt project        |
| `data/`    | manifests, seeds, static files |
| `plugins/` | Python libs & dbt installation |


#### 3. Deploying dbt inside Cloud Composer

- Packaging dbt into Cloud Composer

- Adding dbt-bigquery

- Ensuring all dependencies match Python version in Composer

- Moving dbt project into dags/ so Airflow can execute dbt commands


#### 4. Orchestrating dbt via Airflow

The DAG dynamically creates a task per dbt model, using ```manifest.json```. This ensures proper dependencies between staging â†’ dim â†’ fact models.


```yml
dbt_tasks[node_id] = BashOperator(
    task_id=task_id,
    bash_command=(
        "cd /home/airflow/gcs/dags/ && "
        "dbt run "
        f"--models {node_info['name']} "
        "--target dev "
        # "--full-refresh " Drop and recreate ALL â€œtableâ€ and â€œincrementalâ€ models
    ),
)
```

#### 5. BigQuery Configuration

- Raw dataset: ```olist_raw```

- Transformed dataset: ```olist_dbt```

- OAuth authentication for local dbt

- Service account impersonation for Airflow execution



---
#

âœ… Design and implement ELT data pipelines
âœ… A dbt project with tests, docs, and lineage
âœ… Designing partitioned and clustered tables (BigQuery)
âœ… Query optimization
âœ… Setting up CI/CD to run dbt automatically on push
âœ… Scheduling dbt jobs with Airflow

---

# ğŸ› ï¸ Problems and Solutions : Airflow + dbt + BigQuery Integration

This section documents the major issues encountered while orchestrating dbt models through Cloud Composer (Airflow) with BigQuery and GCS, and how each was diagnosed and resolved.

### Composer couldnâ€™t impersonate service account


#### Error
```yml
iam.serviceAccounts.getAccessToken denied
```
#### Solution
Grant Airflow Worker SA

# TODO
```bash
gcloud

```


### 1ï¸âƒ£ Airflow Could Not Find manifest.json

#### Error
```yml
FileNotFoundError: [Errno 2] No such file or directory: '/home/airflow/gcs/dags/target/manifest.json'
```
#### Cause
The dbt project was not copied correctly into the GCS bucket used by Cloud Composer.
Missing files and folders under 'dag' folder :

- models/model_file_name.sql etc ...
- dbt_project.yml
- sources files (_src.yml)
- target/manifest.json

#### Solution

```bash
gs://<composer-bucket>/dags/
```

```yml
dags/
 â”œâ”€â”€ dbt_dag.py
 â”œâ”€â”€ dbt_project.yml
 â”œâ”€â”€ models/
      â”œâ”€â”€staging/
      â”œâ”€â”€marts/
 â”œâ”€â”€ macros/
 â”œâ”€â”€ seeds/
 â”œâ”€â”€ target/manifest.json
```

### 2ï¸âƒ£ dbt Could Not Find Sources in BigQuery
dbt sources failing inside Composer
#### Error

```bash
Model ... depends on a source named 'olist_raw.orders' which was not found
```

#### Cause
sources YAML file (_src.yml) was not uploaded to GCS â†’ Composer environment.

#### Solution

```yml
dags/
â”œâ”€â”€ models/
  â”œâ”€â”€staging/
  â”œâ”€â”€marts/
  â”œâ”€â”€_src.yml
```

### 3ï¸âƒ£ Profiles Directory Not Found

#### Error

```yml
Invalid value for '--profiles-dir': Path 'home/airflow/gcs/data/profiles' does not exist.
```
#### Cause
profiles.yml was missing

#### Solution

```yml
dags/
  â”œâ”€â”€ models/
data/
  â”œâ”€â”€ profiles/
    â”œâ”€â”€profiles.yml
```
```bash
gs://<composer-bucket>/data/profiles/profiles.yml
```


### 4ï¸âƒ£ Duplicate Model Names

#### Error
There were 2 exactly the same sql file but in the differnt file location. dbt pick up all sql files from 'models/'

```bash
dbt found two models with the name "stg_olist_product_src".(from manual work trigger project, instead of using Airflow)
...
- models/staging/stg_olist_product_src.sql
- models/stg_olist_product_src.sql
```

#### Solution

Removed the duplicate file

```bash
gsutil rm gs://<composer-bucket>/dags/models/stg_olist_product_src.sql
```

Before
```yml
dags/
â”œâ”€â”€ models/
  â”œâ”€â”€staging/stg_olist_product_src.sql
  â”œâ”€â”€stg_olist_product_src.sql
```
â¬‡ï¸

After
```yml
dags/
â”œâ”€â”€ models/
  â”œâ”€â”€staging/stg_olist_product_src.sql
```

### 5ï¸âƒ£ View vs Table Conflict in BigQuery

#### Error
```yml
Trying to create view `olist_dbt.dim_product_translated_name`,
but it currently exists as a TABLE.
```

#### Cause
This model had previously been materialized as a **table** when dbt was run locally.
Inside Composer, dbt tried to create it as a **view** (based on **dbt_project.yml** config), leading to a conflict.

#### Solution

Manually delete table(olist_dbt.dim_product_translated_name) from BigQuery


### ğŸ“Š Outcome: Fully Working Production-Style Pipeline
âœ” Airflow schedules dbt runs

âœ” dbt models are executed in dependency order

âœ” Raw â†’ Staging â†’ Dim â†’ Fact models are materialized in BigQuery

âœ” Airflow retries, logs, and error handling fully operational

âœ” IAM-secure environment using impersonation

âœ” GCS syncing ensures fully automated deployments


# What I Learnt From Issues

- GCS â†” Composer file sync
- Managing distributed systems (Airflow + dbt + GCS + BigQuery)
- Deep understanding of IAM, impersonation, and service account auth
- Debugging complex Airflow â†’ dbt subprocess errors
- Cloud Composer file system structure
- Using dbtâ€™s manifest.json to build DAGs
- Organising GCS buckets for automated DAG deployment
- Running dbt Core in production-grade cloud environments

---
