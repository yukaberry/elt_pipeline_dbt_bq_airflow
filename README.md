# âš™ï¸ Project : Modern Data ELT pipele for e-commerce dataset, olist, with dbt, BigQuery and Airflow (Cloud Composer)

ğŸš€ End-to-End Production-Grade Orchestration on Google Cloud

This project demonstrates a fully automated **ELT pipeline** built using **dbt, Airflow (Cloud Composer), Google Cloud Storage, and BigQuery**. It highlights modern data engineering practices including orchestration, CI/CD automation, dependency management, dbt modeling, and scalable warehouse design.

The pipeline is built to match real production standards used by data teams.

# ğŸ§±ğŸ—ï¸ Architecture Overview

```yml
Google Cloud Composer (Airflow)
        â”‚
        â”‚  Schedules + Orchestrates
        â–¼
Airflow DAG â†’ BashOperator â†’ dbt Core
        â”‚
        â”‚  SQL Transformations + Tests
        â–¼
 BigQuery (raw â†’ staging â†’ marts)
        â”‚
        â–¼
 Curated analytical tables

```

â˜ï¸ GCS and Cloud Composer : Assets are synchronized through GCS buckets used by Cloud Composer

```yml
/home/airflow/gcs/dags/      â†’ Airflow DAG code
/home/airflow/gcs/plugins/   â†’ dbt + Python dependencies
/home/airflow/gcs/data/      â†’ dbt project (models/, sources/, manifest.json)
```

# ğŸ¯ Target

- Build a fully **automated ELT pipeline** using **dbt**

- **Orchestrate** dbt models with **Airflow** on **Cloud Composer**

- Use **BigQuery** as the warehouse for both raw and transformed data

- Implement **service account** impersonation, OAuth, and IAM best practices

- Solve real **production** issues in distributed cloud environments

- Produce an **end-to-end** portfolio-ready **data engineering** system

# ğŸ§® Key Features

### Orchestration & Automation

âœ´ï¸ Cloud Composer (Managed Airflow) used to orchestrate dbt workflows end-to-end.

âœ´ï¸ Automated dbt model execution via Airflow DAGs (staging â†’ dims â†’ marts).

âœ´ï¸ Dependency resolution via ```manifest.json```, ensuring correct execution order.

âœ´ï¸ Retries, timeouts, logging, and error handling for production readiness.

### Deployment & Infrastructure

âœ´ï¸ GCS bucket code syncing used for deploying dbt models and DAG files to Composer.

âœ´ï¸ IAM-secure service account impersonation for least-privilege access to BigQuery and GCS.

### dbt Data Modeling

âœ´ï¸ BigQuery as the warehouse with:**Source models, Staging models (materialized as views), Dim / mart models (materialized as tables)**

âœ´ï¸ dbt tests, documentation, lineage, and model configurations.

âœ´ï¸ Support for incremental, full-refresh, and dependency-driven execution.

### BigQuery Engineering

âœ´ï¸ Partitioned and clustered table design for efficient cost-optimised querying.

âœ´ï¸ SQL transformations optimised for scalable analytical workloads.

âœ´ï¸ Performance-aware modeling following ELT best practices.





# ğŸ”§ Cloud Components Used

| Component          | Purpose                                |
| ------------------ | -------------------------------------- |
| **Cloud Composer** | Orchestration (managed Airflow)        |
| **BigQuery**       | Data warehouse                         |
| **dbt Core**       | SQL transformations, lineage, tests    |
| **GCS Buckets**    | Store DAGs, dbt project, logs          |
| **IAM**            | Secure authentication between services |

# ğŸ“ Folder Structure in GCS


```yml
gs://<composer-bucket>/dags/
    â”œâ”€â”€ dbt_dag.py
    â”œâ”€â”€ models/
        â”œâ”€â”€ marts/
            â”œâ”€â”€dim_product_translated_name.sql
        â”œâ”€â”€ staging/
            â”œâ”€â”€stg_olist_product_src.sql
            â”œâ”€â”€stg_translated_product_name_src.sql

    â”œâ”€â”€ macros/
        â”œâ”€â”€ norm_key.sql
    â”œâ”€â”€ dbt_project.yml
    â””â”€â”€ _src.yml


gs://<raw-data-bucket>/data/
    â”œâ”€â”€ profiles/
        â”œâ”€â”€ profiles.yml
    â”œâ”€â”€ target/
        â”œâ”€â”€manifest.json

```



# What I build

#### 1. Cloud Composer Environment Setup
I fully configured Composer, including:

- Environment creation in europe-west1
- Determining high-resilience vs standard resilience
- Installing Python/dbt dependencies through PyPI
- Configuring Composer Workers to run dbt


#### 2. GCS Bucket Architecture

| Folder     | Purpose                        |
| ---------- | ------------------------------ |
| `dags/`    | DAG files + dbt project        |
| `data/`    | manifests, seeds, static files |
| `plugins/` | Python libs & dbt installation |


#### 3. Deploying dbt inside Cloud Composer

- Packaging dbt into Cloud Composer

- Adding dbt-bigquery

- Ensuring all dependencies match Python version in Composer

- Moving dbt project into dags/ so Airflow can execute dbt commands


#### 4. Orchestrating dbt via Airflow

The DAG dynamically creates a task per dbt model, using ```manifest.json```. This ensures proper dependencies between staging â†’ dim â†’ fact models.


```yml
dbt_tasks[node_id] = BashOperator(
    task_id=task_id,
    bash_command=(
        "cd /home/airflow/gcs/dags/ && "
        "dbt run "
        f"--models {node_info['name']} "
        "--target dev "
        # "--full-refresh " Drop and recreate ALL â€œtableâ€ and â€œincrementalâ€ models
    ),
)
```

#### 5. BigQuery Configuration

- Raw dataset: ```olist_raw```

- Transformed dataset: ```olist_dbt```

- OAuth authentication for local dbt

- Service account impersonation for Airflow execution

---

# ğŸ› ï¸ Problems and Solutions : Airflow + dbt + BigQuery Integration

This section documents the major issues encountered while orchestrating dbt models through Cloud Composer (Airflow) with BigQuery and GCS, and how each was diagnosed and resolved.

### Composer couldnâ€™t impersonate service account


#### Error
```yml
iam.serviceAccounts.getAccessToken denied
```
#### Solution
Grant Airflow Worker SA, run this command on the terminal.


```bash

gcloud iam service-accounts add-iam-policy-binding \
  dbt-olist-project@< project ID here >.iam.gserviceaccount.com \
  --member="<your emial address >@gmail.com" \
  --role="roles/iam.serviceAccountTokenCreator"


```


### 1ï¸âƒ£ Airflow Could Not Find manifest.json

#### Error
```yml
FileNotFoundError: [Errno 2] No such file or directory: '/home/airflow/gcs/dags/target/manifest.json'
```
#### Cause
The dbt project was not copied correctly into the GCS bucket used by Cloud Composer.
Missing files and folders under 'dag' folder :

- models/model_file_name.sql etc ...
- dbt_project.yml
- sources files (_src.yml)
- target/manifest.json

#### Solution

```bash
gs://<composer-bucket>/dags/
```

```yml
dags/
 â”œâ”€â”€ dbt_dag.py
 â”œâ”€â”€ dbt_project.yml
 â”œâ”€â”€ models/
      â”œâ”€â”€staging/
      â”œâ”€â”€marts/
 â”œâ”€â”€ macros/
 â”œâ”€â”€ seeds/
 â”œâ”€â”€ target/manifest.json
```

### 2ï¸âƒ£ dbt Could Not Find Sources in BigQuery
dbt sources failing inside Composer
#### Error

```bash
Model ... depends on a source named 'olist_raw.orders' which was not found
```

#### Cause
sources YAML file (_src.yml) was not uploaded to GCS â†’ Composer environment.

#### Solution

```yml
dags/
â”œâ”€â”€ models/
  â”œâ”€â”€staging/
  â”œâ”€â”€marts/
  â”œâ”€â”€_src.yml
```

### 3ï¸âƒ£ Profiles Directory Not Found

#### Error

```yml
Invalid value for '--profiles-dir': Path 'home/airflow/gcs/data/profiles' does not exist.
```
#### Cause
profiles.yml was missing

#### Solution

```yml
dags/
  â”œâ”€â”€ models/
data/
  â”œâ”€â”€ profiles/
    â”œâ”€â”€profiles.yml
```
```bash
gs://<composer-bucket>/data/profiles/profiles.yml
```


### 4ï¸âƒ£ Duplicate Model Names

#### Error
There were 2 exactly the same sql file but in the differnt file location. dbt pick up all sql files from 'models/'

```bash
dbt found two models with the name "stg_olist_product_src".(from manual work trigger project, instead of using Airflow)
...
- models/staging/stg_olist_product_src.sql
- models/stg_olist_product_src.sql
```

#### Solution

Removed the duplicate file

```bash
gsutil rm gs://<composer-bucket>/dags/models/stg_olist_product_src.sql
```

Before
```yml
dags/
â”œâ”€â”€ models/
  â”œâ”€â”€staging/stg_olist_product_src.sql
  â”œâ”€â”€stg_olist_product_src.sql
```
â¬‡ï¸

After
```yml
dags/
â”œâ”€â”€ models/
  â”œâ”€â”€staging/stg_olist_product_src.sql
```

### 5ï¸âƒ£ View vs Table Conflict in BigQuery

#### Error
```yml
Trying to create view `olist_dbt.dim_product_translated_name`,
but it currently exists as a TABLE.
```

#### Cause
This model had previously been materialized as a **table** when dbt was run locally.
Inside Composer, dbt tried to create it as a **view** (based on **dbt_project.yml** config), leading to a conflict.

#### Solution

Manually delete table(olist_dbt.dim_product_translated_name) from BigQuery

### 6. Airflow Timeout & Zombie Task Issue

#### Cause
dbt models took longer than the default time limi, Airflow workers stopped while dbt was running long BigQuery jobs. The Airflow scheduler assumed the task died â†’ marked it as zombie.Even though dbt finished and created some views/tables(not compelted) in BigQuery, Airflow still failed the task.

#### How I diagnosed
- Airflow UI showed tasks red (failed) even though BigQuery tables were partly successfully created.
- dbt logs inside the task output showed:```"Completed successfully"```, ```PASS=1 ERROR=0```
- **Cloud Logging** / Scheduler logs showed **zombie** detection
- No actual dbt errors. was **not dbt-related**, but **Airflow execution + timeout** related.

#### Solution

Airflow configuration overrides in Cloud Composer

```yml
[celery]
task_soft_time_limit = 3000
task_time_limit = 36000
worker_prefetch_multiplier = 1

[scheduler]
scheduler_zombie_task_threshold = 600
scheduler_heartbeat_sec = 10
```


dbt models took longer to execute than Airflowâ€™s default task timeout and heartbeat thresholds. **Cloud Composer** uses **CeleryExecutor**, and **Celery enforces strict timeouts**

- ```task_time_limit``` â†’ Maximum wall-clock time allowed for a task

- ```task_soft_time_limit``` â†’ When Celery sends a soft kill signal

- ```scheduler_zombie_task_threshold``` â†’ How long Airflow waits before marking a task as â€œzombieâ€ if it stops sending heartbeats


```python
from datetime import timedelta

dbt_run = BashOperator(
    execution_timeout=timedelta(minutes=40)
    )
```


### ğŸ“Š Outcome: Fully Working Production-Style Pipeline
âœ” Airflow schedules dbt runs

âœ” dbt models are executed in dependency order

âœ” Raw â†’ Staging â†’ Dim â†’ Fact models are materialized in BigQuery

âœ” Airflow retries, logs, and error handling fully operational

âœ” IAM-secure environment using impersonation

âœ” GCS syncing ensures fully automated deployments


# What I Learnt From Issues

- GCS â†” Composer file sync
- Managing distributed systems (Airflow + dbt + GCS + BigQuery)
- Deep understanding of IAM, impersonation, and service account auth
- Debugging complex Airflow â†’ dbt subprocess errors
- Cloud Composer file system structure
- Using dbtâ€™s manifest.json to build DAGs
- Organising GCS buckets for automated DAG deployment
- Running dbt Core in production-grade cloud environments

- shos

---
#
