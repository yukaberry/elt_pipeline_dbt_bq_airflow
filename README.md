# âš™ï¸ Project : Modern Data ELT pipele for e-commerce dataset, olist, with dbt, BigQuery and Airflow (Cloud Composer)

End-to-End Production-Grade Orchestration on Google Cloud

This project implements a complete modern data engineering pipeline, orchestrated with Airflow running on Google Cloud Composer, transforming data using dbt Core, and storing curated data in BigQuery.

The pipeline is built to match real production standards used by data teams.

# ğŸ§±ğŸ—ï¸ Architecture Overview

Google Cloud Composer (Airflow)
        â”‚
        â”‚  Schedules + Orchestrates
        â–¼
Airflow DAG â†’ BashOperator â†’ dbt Core
        â”‚
        â”‚  SQL Transformations + Tests
        â–¼
 BigQuery (raw â†’ staging â†’ marts)
        â”‚
        â–¼
 Curated analytical tables

â˜ï¸ GCS and Cloud Composer : Assets are synchronized through GCS buckets used by Cloud Composer

/home/airflow/gcs/dags/      â†’ Airflow DAG code
/home/airflow/gcs/plugins/   â†’ dbt + Python dependencies
/home/airflow/gcs/data/      â†’ dbt project (models/, sources/, manifest.json)

# ğŸ¯ Target

- Build a fully **automated ELT pipeline** using **dbt**

- **Orchestrate** dbt models with **Airflow** on **Cloud Composer**

- Use **BigQuery** as the warehouse for both raw and transformed data

- Implement **service account** impersonation, OAuth, and IAM best practices

- Solve real **production** issues in distributed cloud environments

- Produce an **end-to-end** portfolio-ready **data engineering** system



#

âœ… Design and implement ELT data pipelines
âœ… A dbt project with tests, docs, and lineage
âœ… Designing partitioned and clustered tables (BigQuery)
âœ… Query optimization
âœ… Setting up CI/CD to run dbt automatically on push
âœ… Scheduling dbt jobs with Airflow



# Problems and Solutions

#
